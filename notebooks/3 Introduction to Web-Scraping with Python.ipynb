{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web-Scraping in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an HTML web-scraper is an easy task if the beforementioned basics of Python programming are properly understood. A very basic understanding of HTML code is also needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Firefox Debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the data and web page that we want to scrape, we most often have to use the debugging software of our browser. In our example we use the debugger of Firefox. To open the debugger, you can visit https://www.immobilienscout24.de/expose/109523308 and press *CTRL+Shift+I* or alternatively Right-Click on the page and select 'Inspect Element'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first data of the page that we are interested in is the rent, or Kaltmiete. To understand where we will find this type of data with our crawler, we can Right-Click on the element and select 'Inspect Elemnt'. The resulting HMTL code should be"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<div class=\"is24qa-kaltmiete is24-value font-semibold\"> 1.437,40 â‚¬ </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can learn here is that the rent element has the HTML class \"is24qa-kaltmiete is24-value font-semibold\", which we can use later in our scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a simple Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing our libraries, we can request the web page of interest. Since we are interested in the content of the web page, we add the function .text from requests to our request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.immobilienscout24.de/expose/109523308').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** After requesting the web page, we have downloaded the complete page and stored it into our variable *r*. From here on out we are working with a local copy of the web page, therefore we do not bother the web page provider with unnecessary requests!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the text of the request easier to use, we need BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now search for specific elements in our text, just as we did in our web browser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_=\"is24qa-kaltmiete is24-value font-semibold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup.find_all(class_=\"is24qa-kaltmiete is24-value font-semibold\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_=\"is24qa-kaltmiete is24-value font-semibold\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_=\"is24qa-kaltmiete is24-value font-semibold\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rent = soup.find_all(class_=\"is24qa-kaltmiete is24-value font-semibold\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You just created your very first web crawler!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting more Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get more data, like the amount of rooms and the square meters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "is24qa-zi is24-value font-semibold\n",
    "is24qa-flaeche is24-value font-semibold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create one function to scrape the complete page and another to get single elements. This way we only request the page once but can search within the requested HTML as often as we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_complete_page(url):\n",
    "    r = requests.get(url).text\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_single_element(soup, html_class): \n",
    "    value = soup.find_all(class_=html_class)[0].text\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = scrape_complete_page('https://www.immobilienscout24.de/expose/109523308')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = extract_single_element(soup, 'is24qa-zi is24-value font-semibold')\n",
    "rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqm = extract_single_element(soup, 'is24qa-flaeche is24-value font-semibold')\n",
    "sqm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_classes = ['is24qa-kaltmiete is24-value font-semibold', \n",
    "                'is24qa-zi is24-value font-semibold', \n",
    "                'is24qa-flaeche is24-value font-semibold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://www.immobilienscout24.de/expose/109523308', \n",
    "        'https://www.immobilienscout24.de/expose/108982092',\n",
    "       'https://www.immobilienscout24.de/expose/110182204']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0]\n",
    "type(urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get only the expose ID, which is the fifth element of this new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0].split('/')[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a function that accepts a list of urls and a list of html classes to automatically download data from multiple listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_elements(urls, html_classes_list):\n",
    "    # create an empty list where all the data is stored\n",
    "    data_all = []\n",
    "    for url in urls:\n",
    "        time.sleep(random.uniform(0.3, 3))\n",
    "        # web page gets requested only once\n",
    "        soup = scrape_complete_page(url)\n",
    "        print('====================================================')\n",
    "        print('url: ')\n",
    "        print(url)\n",
    "        # create an empty list for each data set\n",
    "        data_set = []\n",
    "        data_set.append(url)\n",
    "        # get all elements that are specified in html_classes\n",
    "        for html_class in html_classes_list:\n",
    "            print(html_class)\n",
    "            print(extract_single_element(soup, html_class))\n",
    "            # add the elements to the list\n",
    "            data_set.append(extract_single_element(soup, html_class))\n",
    "        # add all the data into the data_all list as list of lists\n",
    "        data_all.append(data_set)\n",
    "    print(data_all)\n",
    "    # create a pandas dataframe to easily store the data as a .csv-file\n",
    "    column_names = ['url', 'rent', 'rooms','area']\n",
    "    df = pd.DataFrame(data_all, columns = column_names)\n",
    "    df.to_csv('./rent_data.csv', sep=';')\n",
    "    \n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_elements(urls, html_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "id=\"fullscreenGallery\"\n",
    "\n",
    "<img class=\"sp-image \" data-ng-non-bindable=\"\" data-src=\"https://pictures.immobilienscout24.de/listings/6edc5643-2874-4f64-a18d-0ba47a3da32b-1271013993.jpg/ORIG/resize/1106x830%3E/format/jpg/quality/80\" data-caption=\"Wohnzimmer\" data-virtualtourlinkid=\"1\" alt=\"Wohnzimmer\" src=\"//www.static-immobilienscout24.de/statpic/3eaf17869bb51bf27bd7c91bc9853973_pixel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading images requires a request for each image. We also need to find the image-links on the website before sending our requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = scrape_complete_page('https://www.immobilienscout24.de/expose/109523308')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_='sp-image ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_='sp-image ')[0]['data-src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_='sp-image ')[0]['data-src'].split('/ORIG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(class_='sp-image ')[0]['data-src'].split('/ORIG')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = soup.find_all(class_='sp-image ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_urls = []\n",
    "for image in images:\n",
    "    print(image['data-src'].split('/ORIG')[0])\n",
    "    images_urls.append(image['data-src'].split('/ORIG')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(url, images_list):\n",
    "    # get expose id from the URL\n",
    "    expose = url.split('/')[4]\n",
    "    print(\"crawling pictures for expose #: \" + str(expose))\n",
    "    i = 0\n",
    "    if not os.path.exists(\"./images/\"):\n",
    "        os.makedirs(\"./images/\")\n",
    "    for image_url in images_list:\n",
    "        sys.stdout.write('\\r'+\"downloading image # \" + str(i))\n",
    "\n",
    "        r = requests.get(image_url)\n",
    "        #print(image_url)\n",
    "        if not os.path.exists(\"./images/\" + expose + \"/\"):\n",
    "            os.makedirs(\"./images/\" + expose + \"/\")\n",
    "        with open(\"./images/\" + expose + \"/\" + str(i) + \".jpg\", \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        i = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend the scraping function with image saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the option to save images to our previous function. We also add a try and except method to catch any potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_elements(urls, html_classes_list):\n",
    "    # create an empty list where all the data is stored\n",
    "    data_all = []\n",
    "    \n",
    "    for url in urls:\n",
    "        # a short random break between requests is very important to not be a bother to the \n",
    "        # web service provider\n",
    "        time.sleep(random.uniform(0.3, 2))\n",
    "        # here we added a try and except to skip errors with pages that are not \n",
    "        # standard to the regular layout of immobilienscout.de\n",
    "        try:\n",
    "            # web page gets requested only once\n",
    "            soup = scrape_complete_page(url)\n",
    "            print('\\n')\n",
    "            print('====================================================')\n",
    "            print('url: ' + str(url))\n",
    "            # create an empty list for each data set\n",
    "            data_set = []\n",
    "            data_set.append(url)\n",
    "            # get all elements that are specified in html_classes\n",
    "            for html_class in html_classes_list:\n",
    "                # print(html_class)\n",
    "                # print(extract_single_element(soup, html_class))\n",
    "                # add the elements to the list\n",
    "                data_set.append(extract_single_element(soup, html_class))\n",
    "            # add all the data into the data_all list as list of lists\n",
    "            data_all.append(data_set)\n",
    "\n",
    "            # new code to save images from all urls as well as the data from \n",
    "            # before\n",
    "            images = soup.find_all(class_='sp-image ')\n",
    "            images_urls = []\n",
    "            for image in images:\n",
    "                images_urls.append(image['data-src'].split('/ORIG')[0])\n",
    "            save_images(url, images_urls)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print(data_all)\n",
    "    # create a pandas dataframe to easily store the data as a .csv-file\n",
    "    column_names = ['url', 'rent', 'rooms','area']\n",
    "    df = pd.DataFrame(data_all, columns = column_names)\n",
    "    df.to_csv('./rent_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_elements(urls, html_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get multiple listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal we want to achieve is to automatically get all the listings of a specific city and crawl the data as well as the images that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,5):\n",
    "    url = f'https://www.immobilienscout24.de/Suche/S-T/P-{i}/Wohnung-Miete/Umkreissuche/Berlin/-/229459/2511140/-/-/50?enteredFrom=result_list'\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.immobilienscout24.de/Suche/S-T/P-1/Wohnung-Miete/Umkreissuche/Berlin/-/229459/2511140/-/-/50?enteredFrom=result_list'\n",
    "r = requests.get(url)\n",
    "data = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(data)\n",
    "urls = soup.find_all('article')\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0]['data-obid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listings_urls():\n",
    "    columns = ['url']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    url_list =[]\n",
    "    pagelimit = 3\n",
    "    for i in range(1, pagelimit):\n",
    "        url = f'https://www.immobilienscout24.de/Suche/S-T/P-{i}/Wohnung-Miete/Umkreissuche/Berlin/-/229459/2511140/-/-/50?enteredFrom=result_list'\n",
    "        r = requests.get(url)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data)\n",
    "        urls = soup.find_all('article')\n",
    "        j = 0\n",
    "        for expose in urls:\n",
    "            j = j + 1\n",
    "            new_url = 'https://www.immobilienscout24.de/expose/' + str(expose['data-obid'])\n",
    "            url_list.append(new_url)\n",
    "    return url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = listings_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_elements(urls[0:10], html_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes web sites block people who are scraping their sites. Therefore we have to use proxies to disguise our identity. In this case we use very slow free proxies, but if you actually need proxies for your project, you should probably invest into some professionial service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the free proxies list once and save it locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\n",
    "\"https://proxyscrape.com/api?request=getproxies&proxytype=http&timeout=\")\n",
    "with open(\"./proxies.txt\", \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./proxies.txt\", \"r\") as f:\n",
    "    proxies = f.read().splitlines()\n",
    "    proxies = list(proxies)\n",
    "    # print(proxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = requests.get('https://api.ipify.org').text\n",
    "print('My public IP address is:', ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy={'https': proxies[5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy={'https': proxies[5]}\n",
    "ip = requests.get('https://api.ipify.org', proxies = proxy).text\n",
    "print('My public IP address is:', ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_with_proxy(request_url):\n",
    "    with open(\"./proxies.txt\", \"r\") as f:\n",
    "        proxies = f.read().splitlines()\n",
    "        proxies = list(proxies)\n",
    "        \n",
    "    try:\n",
    "        proxy = {'https': proxies[random.randint(0, len(proxies))]}\n",
    "        r = requests.get(request_url, timeout=1.0, proxies = proxy)\n",
    "        # print(r.text)\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        return request_with_proxy(request_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_with_proxy('https://api.ipify.org').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler with Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_complete_page_with_proxy(url):\n",
    "    r = request_with_proxy(url).text\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_with_proxy(url, images_list):\n",
    "    # get expose id from the URL\n",
    "    expose = url.split('/')[4]\n",
    "    print(\"crawling pictures for expose #: \" + str(expose))\n",
    "    i = 0\n",
    "    if not os.path.exists(\"./images/\"):\n",
    "        os.makedirs(\"./images/\")\n",
    "    for image_url in images_list:\n",
    "        sys.stdout.write('\\r'+\"downloading image # \" + str(i))\n",
    "\n",
    "        r = request_with_proxy(image_url)\n",
    "        #print(image_url)\n",
    "        if not os.path.exists(\"./images/\" + expose + \"/\"):\n",
    "            os.makedirs(\"./images/\" + expose + \"/\")\n",
    "        with open(\"./images/\" + expose + \"/\" + str(i) + \".jpg\", \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_elements_with_proxy(urls, html_classes_list):\n",
    "    # create an empty list where all the data is stored\n",
    "    data_all = []\n",
    "    \n",
    "    for url in urls:\n",
    "        # here we added a try and except to skip errors with pages that are not \n",
    "        # standard to the regular layout of immobilienscout.de\n",
    "        try:\n",
    "            # web page gets requested only once\n",
    "            soup = scrape_complete_page_with_proxy(url)\n",
    "            print('\\n')\n",
    "            print('====================================================')\n",
    "            print('url: ' + str(url))\n",
    "            # create an empty list for each data set\n",
    "            data_set = []\n",
    "            data_set.append(url)\n",
    "            # get all elements that are specified in html_classes\n",
    "            for html_class in html_classes_list:\n",
    "                # print(html_class)\n",
    "                # print(extract_single_element(soup, html_class))\n",
    "                # add the elements to the list\n",
    "                data_set.append(extract_single_element(soup, html_class))\n",
    "            # add all the data into the data_all list as list of lists\n",
    "            data_all.append(data_set)\n",
    "\n",
    "            # new code to save images from all urls as well as the data from \n",
    "            # before\n",
    "            images = soup.find_all(class_='sp-image ')\n",
    "            images_urls = []\n",
    "            for image in images:\n",
    "                images_urls.append(image['data-src'].split('/ORIG')[0])\n",
    "            save_images_with_proxy(url, images_urls)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    print(data_all)\n",
    "    # create a pandas dataframe to easily store the data as a .csv-file\n",
    "    column_names = ['url', 'rent', 'rooms','area']\n",
    "    df = pd.DataFrame(data_all, columns = column_names)\n",
    "    df.to_csv('./rent_data.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_elements_with_proxy(urls, html_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
